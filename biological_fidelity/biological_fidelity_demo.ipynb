{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "natural-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import sys\n",
    "from sklearn.metrics import fbeta_score\n",
    "import logging\n",
    "import numpy as np\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "characteristic-theater",
   "metadata": {},
   "source": [
    "## self-inhibtiing neuron model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "illegal-violin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heaviside step function\n",
    "class STEFunction(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        output = torch.gt(input, 0.)\n",
    "        return output.float()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        x = torch.tanh(input)\n",
    "        fu = 1.0 - x.mul(x)\n",
    "        return grad_input * fu\n",
    "\n",
    "spikeplus = STEFunction.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "interim-sociology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-inhibtiing neuron model\n",
    "class nerve_cell(nn.Module):\n",
    "    def __init__(self, neuron_num, activation=spikeplus):\n",
    "        super(nerve_cell, self).__init__()\n",
    "        self.num = neuron_num\n",
    "        self._activation = activation\n",
    "        self._g = nn.LeakyReLU(0.1)\n",
    "\n",
    "        # define inner weight trainable parameter\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(self.num, 1))\n",
    "        self.weight = nn.init.xavier_normal_(self.weight, gain=1)\n",
    "\n",
    "        # define outer attribute parameter\n",
    "        self.tau = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.tau = nn.init.constant_(self.tau, 0.2)\n",
    "\n",
    "        self.C = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.C = nn.init.constant_(self.C, 1.0)\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.gamma = nn.init.constant_(self.gamma, 0.05)\n",
    "\n",
    "        self.U_rest = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.U_rest = nn.init.constant_(self.U_rest, 0.0)\n",
    "\n",
    "        self.U_th = nn.Parameter(torch.FloatTensor(1))\n",
    "        self.U_th = nn.init.constant_(self.U_th, -1.0)\n",
    "\n",
    "        # define the state variable\n",
    "        self.U = self.U_rest.data.clone().cuda()\n",
    "        self.s = torch.FloatTensor([0.]).cuda()\n",
    "\n",
    "    def reset(self):\n",
    "        self.U = self.U_rest.data.clone().cuda()\n",
    "        self.s = torch.FloatTensor([0.]).cuda()\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "              input: of shape (batch_size, neuron_num)\n",
    "        Outputs:\n",
    "              s: of shape (batch_size, neuron_num)\n",
    "        \"\"\"\n",
    "        # reset\n",
    "        self.U = torch.mul(self.U, 1.0 - self.s) + torch.mul(self.U_rest, self.s)\n",
    "        # external stimuli\n",
    "        I = torch.matmul(input, self.weight)\n",
    "        # input considering inhibiting-autapse\n",
    "        I = torch.add(I, - self.gamma.mul(self.s))\n",
    "        # potential dynamics\n",
    "        I = torch.mul(I, self.C)\n",
    "        self.U = torch.mul(self.U, (1.0 - self.tau))  # decay\n",
    "        self.U = torch.add(self.U, I)\n",
    "        self.U = torch.add(self.U, torch.mul(self.tau, self.U_rest))\n",
    "        self.U = self._g(self.U)\n",
    "        # output\n",
    "        self.s = self._activation(self.U + self.U_th)\n",
    "\n",
    "        return self.s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "committed-psychiatry",
   "metadata": {},
   "source": [
    "## surrogate neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "consistent-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "class brain_network(nn.Module):\n",
    "    def __init__(self, num, criterion = nn.MSELoss()):\n",
    "        super(brain_network, self).__init__()\n",
    "        self.n = num\n",
    "        self.criterion = criterion\n",
    "\n",
    "        self.layer = nerve_cell(self.n)\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs, timesteps, ins = input.size()\n",
    "        self.layer.reset()\n",
    "        self.layer.tau.data.clamp_(0.2 - 0.1, 0.2 + 0.1)\n",
    "        self.layer.C.data.clamp_(1.0 - 0.1, 1.0 + 0.1)\n",
    "        self.layer.gamma.data.clamp_(0.05 - 0.025, 0.05 + 0.025)\n",
    "        self.layer.U_rest.data.clamp_(0.0 - 0.5, 0.0 + 0.5)\n",
    "        self.layer.U_th.data.clamp_(-1.0 - 0.5, -1.0 + 0.5)\n",
    "        output = 0\n",
    "\n",
    "        for t in range(timesteps):\n",
    "            x = self.layer(input[:, t, :])\n",
    "            output = x\n",
    "        return output\n",
    "    \n",
    "    def loss(self, spiking, target):\n",
    "        # spiking_rate, [batch_size, output_size]\n",
    "        #spiking_rate = torch.mean(spiking, dim=1).type(torch.FloatTensor).cuda()\n",
    "        spiking_rate = spiking.reshape(-1)\n",
    "        loss = self.criterion(spiking_rate, target)\n",
    "\n",
    "        prediction = spiking_rate.clone()\n",
    "        prediction = prediction.cpu().detach().numpy()\n",
    "        label = target.clone()\n",
    "        label = label.cpu().detach().numpy()\n",
    "        macro_f = fbeta_score(label, prediction, average='macro', beta=1)\n",
    "\n",
    "        return loss, macro_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "varying-taxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = 1 # choose the simulated neuron \n",
    "gpu = 0\n",
    "seed = 0\n",
    "neuron_num = 28\n",
    "bs = 32\n",
    "lr = 0.05\n",
    "lr_min = 1e-5\n",
    "wd = 3e-4\n",
    "epochs = 300\n",
    "grad_clip = 5\n",
    "report_freq = 5\n",
    "data = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "covered-badge",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgrageMeter(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt\n",
    "\n",
    "def train(train_queue, model, optimizer):\n",
    "    obj_loss = AvgrageMeter()\n",
    "    obj_ma_f1 = AvgrageMeter()\n",
    "    \n",
    "    neurons = [i for i in range(29)]\n",
    "    neurons.remove(neuron)\n",
    "    \n",
    "    model.train()\n",
    "    for step, (input, target) in enumerate(train_queue):\n",
    "        n = input.size(0)\n",
    "        input = input[:, :, neurons]\n",
    "        target = target[:, neuron]\n",
    "        input = Variable(input, requires_grad=False).type(torch.FloatTensor).cuda()\n",
    "        target = Variable(target, requires_grad=False).type(torch.FloatTensor).cuda()\n",
    "\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        output = model(input)\n",
    "\n",
    "        loss, macro_f1= model.loss(output, target)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        obj_loss.update(loss.item(), n)\n",
    "        obj_ma_f1.update(macro_f1.item(), n)\n",
    "\n",
    "\n",
    "        if step % report_freq == 0:\n",
    "            logging.info('train %03d %e %e', step, obj_loss.avg, obj_ma_f1.avg)\n",
    "    return obj_loss.avg, obj_ma_f1.avg\n",
    "\n",
    "def infer(valid_queue, model):\n",
    "    obj_loss = AvgrageMeter()\n",
    "    obj_ma_f1 = AvgrageMeter()\n",
    "    \n",
    "    neurons = [i for i in range(29)]\n",
    "    neurons.remove(neuron)\n",
    "    \n",
    "    regression = []\n",
    "    for step, (input, target) in enumerate(valid_queue):\n",
    "        n = input.size(0)\n",
    "        input = input[:,:,neurons]\n",
    "        target = target[:,neuron]\n",
    "        input = input.type(torch.FloatTensor).cuda()\n",
    "        target = target.type(torch.FloatTensor).cuda()\n",
    "\n",
    "        output = model(input)\n",
    "        loss, macro_f1 = model.loss(output, target)\n",
    "\n",
    "        out = output.reshape(-1)\n",
    "        out = out.cpu().detach().numpy()\n",
    "        regression.extend(out)\n",
    "\n",
    "        obj_loss.update(loss.item(), n)\n",
    "        obj_ma_f1.update(macro_f1.item(), n)\n",
    "\n",
    "    return obj_loss.avg, obj_ma_f1.avg, regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d338cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO, format=log_format, datefmt='%m/%d %I:%M:%S %p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "satisfactory-nothing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:56:24 PM gpu device = 0\n"
     ]
    }
   ],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logging.info('no gpu device available')\n",
    "    sys.exit(1)\n",
    "torch.cuda.set_device(gpu)\n",
    "torch.manual_seed(seed)\n",
    "cudnn.enabled = True\n",
    "torch.cuda.manual_seed(seed)\n",
    "logging.info('gpu device = %s' % gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "actual-blocking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model:\n"
     ]
    }
   ],
   "source": [
    "print(\"load model:\")\n",
    "criterion = nn.MSELoss()\n",
    "criterion = criterion.cuda()\n",
    "model = brain_network(num=neuron_num, criterion=criterion).cuda()\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "northern-dayton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load dataset\n",
      "DataLoader:\n"
     ]
    }
   ],
   "source": [
    "print('load dataset')\n",
    "x_train = np.load(data + 'x_train.npy')\n",
    "y_train = np.load(data + 'y_train.npy')\n",
    "x_test = np.load(data + 'x_test.npy')\n",
    "y_test = np.load(data + 'y_test.npy')\n",
    "\n",
    "x_train = torch.from_numpy(x_train)\n",
    "x_test = torch.from_numpy(x_test)\n",
    "y_train = torch.from_numpy(y_train)\n",
    "y_test = torch.from_numpy(y_test)\n",
    "\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "print(\"DataLoader:\")\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=bs,\n",
    "    pin_memory=True)\n",
    "\n",
    "test_queue = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=100,\n",
    "    pin_memory=True)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, epochs, eta_min=lr_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "clear-track",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:56:25 PM epoch 0 lr 5.000000e-02\n",
      "06/08 11:56:25 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:25 PM train 005 6.250000e-02 6.500787e-01\n",
      "06/08 11:56:25 PM train_loss 0.090000, train_f1 0.601957\n",
      "06/08 11:56:25 PM epoch 1 lr 4.999863e-02\n",
      "06/08 11:56:25 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:26 PM train 005 6.770833e-02 5.654784e-01\n",
      "06/08 11:56:26 PM train_loss 0.070000, train_f1 0.554819\n",
      "06/08 11:56:26 PM epoch 2 lr 4.999452e-02\n",
      "06/08 11:56:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:26 PM train 005 5.729167e-02 5.683927e-01\n",
      "06/08 11:56:26 PM train_loss 0.060000, train_f1 0.575304\n",
      "06/08 11:56:26 PM epoch 3 lr 4.998767e-02\n",
      "06/08 11:56:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:26 PM train 005 4.687500e-02 6.127902e-01\n",
      "06/08 11:56:27 PM train_loss 0.060000, train_f1 0.599305\n",
      "06/08 11:56:27 PM epoch 4 lr 4.997808e-02\n",
      "06/08 11:56:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:27 PM train 005 4.687500e-02 6.127902e-01\n",
      "06/08 11:56:27 PM train_loss 0.053333, train_f1 0.637029\n",
      "06/08 11:56:27 PM epoch 5 lr 4.996575e-02\n",
      "06/08 11:56:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:27 PM train 005 5.208333e-02 6.667949e-01\n",
      "06/08 11:56:27 PM train_loss 0.073333, train_f1 0.633706\n",
      "06/08 11:56:27 PM epoch 6 lr 4.995068e-02\n",
      "06/08 11:56:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:28 PM train 005 5.208333e-02 6.945789e-01\n",
      "06/08 11:56:28 PM train_loss 0.056667, train_f1 0.709340\n",
      "06/08 11:56:28 PM epoch 7 lr 4.993288e-02\n",
      "06/08 11:56:28 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:28 PM train 005 5.208333e-02 5.281341e-01\n",
      "06/08 11:56:28 PM train_loss 0.050000, train_f1 0.617288\n",
      "06/08 11:56:28 PM epoch 8 lr 4.991234e-02\n",
      "06/08 11:56:28 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:29 PM train 005 5.729167e-02 5.684375e-01\n",
      "06/08 11:56:29 PM train_loss 0.063333, train_f1 0.627677\n",
      "06/08 11:56:29 PM epoch 9 lr 4.988907e-02\n",
      "06/08 11:56:29 PM train 000 1.562500e-01 4.576271e-01\n",
      "06/08 11:56:29 PM train 005 8.854167e-02 5.098263e-01\n",
      "06/08 11:56:29 PM train_loss 0.090000, train_f1 0.588205\n",
      "06/08 11:56:29 PM epoch 10 lr 4.986307e-02\n",
      "06/08 11:56:29 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:29 PM train 005 4.166667e-02 7.639315e-01\n",
      "06/08 11:56:30 PM train_loss 0.053333, train_f1 0.723518\n",
      "06/08 11:56:30 PM epoch 11 lr 4.983435e-02\n",
      "06/08 11:56:30 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:30 PM train 005 6.250000e-02 5.667474e-01\n",
      "06/08 11:56:30 PM train_loss 0.056667, train_f1 0.642000\n",
      "06/08 11:56:30 PM epoch 12 lr 4.980291e-02\n",
      "06/08 11:56:30 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:30 PM train 005 5.729167e-02 5.683457e-01\n",
      "06/08 11:56:30 PM train_loss 0.050000, train_f1 0.653449\n",
      "06/08 11:56:30 PM epoch 13 lr 4.976874e-02\n",
      "06/08 11:56:30 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:31 PM train 005 5.729167e-02 5.683457e-01\n",
      "06/08 11:56:31 PM train_loss 0.076667, train_f1 0.567937\n",
      "06/08 11:56:31 PM epoch 14 lr 4.973186e-02\n",
      "06/08 11:56:31 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:31 PM train 005 7.291667e-02 5.223947e-01\n",
      "06/08 11:56:31 PM train_loss 0.066667, train_f1 0.571554\n",
      "06/08 11:56:31 PM epoch 15 lr 4.969227e-02\n",
      "06/08 11:56:31 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:31 PM train 005 6.250000e-02 6.502262e-01\n",
      "06/08 11:56:32 PM train_loss 0.060000, train_f1 0.672834\n",
      "06/08 11:56:32 PM epoch 16 lr 4.964997e-02\n",
      "06/08 11:56:32 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:32 PM train 005 6.250000e-02 6.334161e-01\n",
      "06/08 11:56:32 PM train_loss 0.053333, train_f1 0.695094\n",
      "06/08 11:56:32 PM epoch 17 lr 4.960497e-02\n",
      "06/08 11:56:32 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:32 PM train 005 6.770833e-02 5.651888e-01\n",
      "06/08 11:56:32 PM train_loss 0.063333, train_f1 0.603815\n",
      "06/08 11:56:32 PM epoch 18 lr 4.955727e-02\n",
      "06/08 11:56:32 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:33 PM train 005 5.208333e-02 7.111962e-01\n",
      "06/08 11:56:33 PM train_loss 0.050000, train_f1 0.734448\n",
      "06/08 11:56:33 PM epoch 19 lr 4.950688e-02\n",
      "06/08 11:56:33 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:33 PM train 005 6.250000e-02 5.585529e-01\n",
      "06/08 11:56:33 PM train_loss 0.056667, train_f1 0.619611\n",
      "06/08 11:56:33 PM epoch 20 lr 4.945380e-02\n",
      "06/08 11:56:33 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:34 PM train 005 5.729167e-02 6.432090e-01\n",
      "06/08 11:56:34 PM train_loss 0.053333, train_f1 0.695913\n",
      "06/08 11:56:34 PM epoch 21 lr 4.939804e-02\n",
      "06/08 11:56:34 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:34 PM train 005 5.729167e-02 5.683457e-01\n",
      "06/08 11:56:34 PM train_loss 0.063333, train_f1 0.645201\n",
      "06/08 11:56:34 PM epoch 22 lr 4.933960e-02\n",
      "06/08 11:56:34 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:34 PM train 005 3.645833e-02 7.626101e-01\n",
      "06/08 11:56:35 PM train_loss 0.046667, train_f1 0.774979\n",
      "06/08 11:56:35 PM epoch 23 lr 4.927850e-02\n",
      "06/08 11:56:35 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:35 PM train 005 5.208333e-02 6.780018e-01\n",
      "06/08 11:56:35 PM train_loss 0.050000, train_f1 0.748640\n",
      "06/08 11:56:35 PM epoch 24 lr 4.921474e-02\n",
      "06/08 11:56:35 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:35 PM train 005 5.729167e-02 6.848308e-01\n",
      "06/08 11:56:35 PM train_loss 0.050000, train_f1 0.728000\n",
      "06/08 11:56:35 PM epoch 25 lr 4.914832e-02\n",
      "06/08 11:56:35 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:36 PM train 005 3.645833e-02 7.237198e-01\n",
      "06/08 11:56:36 PM train_loss 0.036667, train_f1 0.755485\n",
      "06/08 11:56:36 PM epoch 26 lr 4.907925e-02\n",
      "06/08 11:56:36 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:36 PM train 005 3.645833e-02 7.514481e-01\n",
      "06/08 11:56:36 PM train_loss 0.053333, train_f1 0.736540\n",
      "06/08 11:56:36 PM epoch 27 lr 4.900754e-02\n",
      "06/08 11:56:36 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:36 PM train 005 6.250000e-02 6.417988e-01\n",
      "06/08 11:56:37 PM train_loss 0.050000, train_f1 0.708802\n",
      "06/08 11:56:37 PM epoch 28 lr 4.893320e-02\n",
      "06/08 11:56:37 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:37 PM train 005 5.729167e-02 6.681641e-01\n",
      "06/08 11:56:37 PM train_loss 0.056667, train_f1 0.717624\n",
      "06/08 11:56:37 PM epoch 29 lr 4.885624e-02\n",
      "06/08 11:56:37 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:37 PM train 005 3.645833e-02 7.653887e-01\n",
      "06/08 11:56:37 PM train_loss 0.060000, train_f1 0.762346\n",
      "06/08 11:56:37 PM epoch 30 lr 4.877666e-02\n",
      "06/08 11:56:37 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:38 PM train 005 5.208333e-02 6.945789e-01\n",
      "06/08 11:56:38 PM train_loss 0.050000, train_f1 0.728790\n",
      "06/08 11:56:38 PM epoch 31 lr 4.869447e-02\n",
      "06/08 11:56:38 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:38 PM train 005 5.208333e-02 6.530018e-01\n",
      "06/08 11:56:38 PM train_loss 0.050000, train_f1 0.706727\n",
      "06/08 11:56:38 PM epoch 32 lr 4.860969e-02\n",
      "06/08 11:56:38 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:38 PM train 005 3.125000e-02 7.390204e-01\n",
      "06/08 11:56:39 PM train_loss 0.030000, train_f1 0.771024\n",
      "06/08 11:56:39 PM epoch 33 lr 4.852231e-02\n",
      "06/08 11:56:39 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:39 PM train 005 3.125000e-02 8.223111e-01\n",
      "06/08 11:56:39 PM train_loss 0.040000, train_f1 0.831807\n",
      "06/08 11:56:39 PM epoch 34 lr 4.843236e-02\n",
      "06/08 11:56:39 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:39 PM train 005 3.645833e-02 7.375129e-01\n",
      "06/08 11:56:40 PM train_loss 0.040000, train_f1 0.707537\n",
      "06/08 11:56:40 PM epoch 35 lr 4.833984e-02\n",
      "06/08 11:56:40 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:40 PM train 005 5.208333e-02 6.946237e-01\n",
      "06/08 11:56:40 PM train_loss 0.063333, train_f1 0.716230\n",
      "06/08 11:56:40 PM epoch 36 lr 4.824476e-02\n",
      "06/08 11:56:40 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:40 PM train 005 2.083333e-02 8.500418e-01\n",
      "06/08 11:56:40 PM train_loss 0.026667, train_f1 0.841175\n",
      "06/08 11:56:40 PM epoch 37 lr 4.814714e-02\n",
      "06/08 11:56:40 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:41 PM train 005 3.645833e-02 7.985831e-01\n",
      "06/08 11:56:41 PM train_loss 0.033333, train_f1 0.809144\n",
      "06/08 11:56:41 PM epoch 38 lr 4.804697e-02\n",
      "06/08 11:56:41 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:41 PM train 005 4.166667e-02 7.223544e-01\n",
      "06/08 11:56:41 PM train_loss 0.043333, train_f1 0.751112\n",
      "06/08 11:56:41 PM epoch 39 lr 4.794428e-02\n",
      "06/08 11:56:41 PM train 000 3.125000e-02 4.920635e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:56:41 PM train 005 3.125000e-02 7.417989e-01\n",
      "06/08 11:56:42 PM train_loss 0.033333, train_f1 0.767056\n",
      "06/08 11:56:42 PM epoch 40 lr 4.783907e-02\n",
      "06/08 11:56:42 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:42 PM train 005 3.645833e-02 7.792320e-01\n",
      "06/08 11:56:42 PM train_loss 0.053333, train_f1 0.776701\n",
      "06/08 11:56:42 PM epoch 41 lr 4.773136e-02\n",
      "06/08 11:56:42 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:42 PM train 005 3.645833e-02 7.404335e-01\n",
      "06/08 11:56:42 PM train_loss 0.030000, train_f1 0.808330\n",
      "06/08 11:56:42 PM epoch 42 lr 4.762115e-02\n",
      "06/08 11:56:42 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:43 PM train 005 4.687500e-02 7.430738e-01\n",
      "06/08 11:56:43 PM train_loss 0.046667, train_f1 0.785617\n",
      "06/08 11:56:43 PM epoch 43 lr 4.750847e-02\n",
      "06/08 11:56:43 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:43 PM train 005 4.687500e-02 7.626131e-01\n",
      "06/08 11:56:43 PM train_loss 0.040000, train_f1 0.786123\n",
      "06/08 11:56:43 PM epoch 44 lr 4.739332e-02\n",
      "06/08 11:56:43 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:43 PM train 005 3.645833e-02 7.930251e-01\n",
      "06/08 11:56:44 PM train_loss 0.036667, train_f1 0.797244\n",
      "06/08 11:56:44 PM epoch 45 lr 4.727571e-02\n",
      "06/08 11:56:44 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:44 PM train 005 3.125000e-02 7.417989e-01\n",
      "06/08 11:56:44 PM train_loss 0.036667, train_f1 0.784801\n",
      "06/08 11:56:44 PM epoch 46 lr 4.715566e-02\n",
      "06/08 11:56:44 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:44 PM train 005 3.125000e-02 8.112463e-01\n",
      "06/08 11:56:44 PM train_loss 0.040000, train_f1 0.820277\n",
      "06/08 11:56:44 PM epoch 47 lr 4.703318e-02\n",
      "06/08 11:56:44 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:45 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:56:45 PM train_loss 0.023333, train_f1 0.871915\n",
      "06/08 11:56:45 PM epoch 48 lr 4.690829e-02\n",
      "06/08 11:56:45 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:45 PM train 005 5.208333e-02 7.500911e-01\n",
      "06/08 11:56:45 PM train_loss 0.050000, train_f1 0.764318\n",
      "06/08 11:56:45 PM epoch 49 lr 4.678099e-02\n",
      "06/08 11:56:45 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:46 PM train 005 4.687500e-02 6.820037e-01\n",
      "06/08 11:56:46 PM train_loss 0.046667, train_f1 0.725288\n",
      "06/08 11:56:46 PM epoch 50 lr 4.665130e-02\n",
      "06/08 11:56:46 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:46 PM train 005 4.687500e-02 6.515887e-01\n",
      "06/08 11:56:46 PM train_loss 0.053333, train_f1 0.712119\n",
      "06/08 11:56:46 PM epoch 51 lr 4.651925e-02\n",
      "06/08 11:56:46 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:46 PM train 005 2.604167e-02 7.820994e-01\n",
      "06/08 11:56:47 PM train_loss 0.026667, train_f1 0.829250\n",
      "06/08 11:56:47 PM epoch 52 lr 4.638483e-02\n",
      "06/08 11:56:47 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:47 PM train 005 3.645833e-02 7.958037e-01\n",
      "06/08 11:56:47 PM train_loss 0.040000, train_f1 0.798120\n",
      "06/08 11:56:47 PM epoch 53 lr 4.624807e-02\n",
      "06/08 11:56:47 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:47 PM train 005 3.125000e-02 8.130883e-01\n",
      "06/08 11:56:47 PM train_loss 0.033333, train_f1 0.836694\n",
      "06/08 11:56:47 PM epoch 54 lr 4.610898e-02\n",
      "06/08 11:56:47 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:48 PM train 005 3.645833e-02 7.931672e-01\n",
      "06/08 11:56:48 PM train_loss 0.036667, train_f1 0.826541\n",
      "06/08 11:56:48 PM epoch 55 lr 4.596757e-02\n",
      "06/08 11:56:48 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:48 PM train 005 5.729167e-02 6.932135e-01\n",
      "06/08 11:56:48 PM train_loss 0.050000, train_f1 0.759974\n",
      "06/08 11:56:48 PM epoch 56 lr 4.582387e-02\n",
      "06/08 11:56:48 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:48 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:56:49 PM train_loss 0.033333, train_f1 0.823040\n",
      "06/08 11:56:49 PM epoch 57 lr 4.567788e-02\n",
      "06/08 11:56:49 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:49 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:56:49 PM train_loss 0.026667, train_f1 0.842979\n",
      "06/08 11:56:49 PM epoch 58 lr 4.552962e-02\n",
      "06/08 11:56:49 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:49 PM train 005 2.604167e-02 8.208979e-01\n",
      "06/08 11:56:49 PM train_loss 0.033333, train_f1 0.831266\n",
      "06/08 11:56:49 PM epoch 59 lr 4.537912e-02\n",
      "06/08 11:56:49 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:50 PM train 005 4.687500e-02 6.766334e-01\n",
      "06/08 11:56:50 PM train_loss 0.050000, train_f1 0.734125\n",
      "06/08 11:56:50 PM epoch 60 lr 4.522638e-02\n",
      "06/08 11:56:50 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:50 PM train 005 4.166667e-02 6.528569e-01\n",
      "06/08 11:56:50 PM train_loss 0.043333, train_f1 0.653357\n",
      "06/08 11:56:50 PM epoch 61 lr 4.507142e-02\n",
      "06/08 11:56:50 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:50 PM train 005 5.729167e-02 6.099676e-01\n",
      "06/08 11:56:51 PM train_loss 0.053333, train_f1 0.677761\n",
      "06/08 11:56:51 PM epoch 62 lr 4.491427e-02\n",
      "06/08 11:56:51 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:51 PM train 005 2.604167e-02 8.264550e-01\n",
      "06/08 11:56:51 PM train_loss 0.030000, train_f1 0.844615\n",
      "06/08 11:56:51 PM epoch 63 lr 4.475492e-02\n",
      "06/08 11:56:51 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:56:51 PM train 005 2.604167e-02 7.820994e-01\n",
      "06/08 11:56:51 PM train_loss 0.033333, train_f1 0.810594\n",
      "06/08 11:56:51 PM epoch 64 lr 4.459342e-02\n",
      "06/08 11:56:52 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:52 PM train 005 3.645833e-02 7.820106e-01\n",
      "06/08 11:56:52 PM train_loss 0.046667, train_f1 0.802475\n",
      "06/08 11:56:52 PM epoch 65 lr 4.442976e-02\n",
      "06/08 11:56:52 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:52 PM train 005 3.125000e-02 8.083759e-01\n",
      "06/08 11:56:52 PM train_loss 0.036667, train_f1 0.801620\n",
      "06/08 11:56:52 PM epoch 66 lr 4.426398e-02\n",
      "06/08 11:56:52 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:53 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:56:53 PM train_loss 0.020000, train_f1 0.884303\n",
      "06/08 11:56:53 PM epoch 67 lr 4.409608e-02\n",
      "06/08 11:56:53 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:53 PM train 005 2.604167e-02 8.319627e-01\n",
      "06/08 11:56:53 PM train_loss 0.030000, train_f1 0.822164\n",
      "06/08 11:56:53 PM epoch 68 lr 4.392609e-02\n",
      "06/08 11:56:53 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:53 PM train 005 2.604167e-02 7.681643e-01\n",
      "06/08 11:56:54 PM train_loss 0.026667, train_f1 0.817735\n",
      "06/08 11:56:54 PM epoch 69 lr 4.375403e-02\n",
      "06/08 11:56:54 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:54 PM train 005 2.604167e-02 7.681643e-01\n",
      "06/08 11:56:54 PM train_loss 0.023333, train_f1 0.826077\n",
      "06/08 11:56:54 PM epoch 70 lr 4.357990e-02\n",
      "06/08 11:56:54 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:54 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:56:54 PM train_loss 0.023333, train_f1 0.871915\n",
      "06/08 11:56:54 PM epoch 71 lr 4.340375e-02\n",
      "06/08 11:56:54 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:55 PM train 005 2.604167e-02 8.126117e-01\n",
      "06/08 11:56:55 PM train_loss 0.036667, train_f1 0.824673\n",
      "06/08 11:56:55 PM epoch 72 lr 4.322557e-02\n",
      "06/08 11:56:55 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:55 PM train 005 3.645833e-02 7.626549e-01\n",
      "06/08 11:56:55 PM train_loss 0.036667, train_f1 0.804417\n",
      "06/08 11:56:55 PM epoch 73 lr 4.304540e-02\n",
      "06/08 11:56:55 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:55 PM train 005 4.687500e-02 7.487697e-01\n",
      "06/08 11:56:56 PM train_loss 0.046667, train_f1 0.785104\n",
      "06/08 11:56:56 PM epoch 74 lr 4.286324e-02\n",
      "06/08 11:56:56 PM train 000 9.375000e-02 4.754098e-01\n",
      "06/08 11:56:56 PM train 005 6.770833e-02 6.071920e-01\n",
      "06/08 11:56:56 PM train_loss 0.060000, train_f1 0.698653\n",
      "06/08 11:56:56 PM epoch 75 lr 4.267913e-02\n",
      "06/08 11:56:56 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:56 PM train 005 3.645833e-02 7.958037e-01\n",
      "06/08 11:56:56 PM train_loss 0.046667, train_f1 0.809319\n",
      "06/08 11:56:56 PM epoch 76 lr 4.249309e-02\n",
      "06/08 11:56:56 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:57 PM train 005 2.083333e-02 8.500418e-01\n",
      "06/08 11:56:57 PM train_loss 0.026667, train_f1 0.864688\n",
      "06/08 11:56:57 PM epoch 77 lr 4.230512e-02\n",
      "06/08 11:56:57 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:57 PM train 005 3.645833e-02 6.988565e-01\n",
      "06/08 11:56:57 PM train_loss 0.043333, train_f1 0.736449\n",
      "06/08 11:56:57 PM epoch 78 lr 4.211525e-02\n",
      "06/08 11:56:57 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:57 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:56:58 PM train_loss 0.026667, train_f1 0.844403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:56:58 PM epoch 79 lr 4.192351e-02\n",
      "06/08 11:56:58 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:58 PM train 005 3.125000e-02 8.166622e-01\n",
      "06/08 11:56:58 PM train_loss 0.030000, train_f1 0.820714\n",
      "06/08 11:56:58 PM epoch 80 lr 4.172992e-02\n",
      "06/08 11:56:58 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:58 PM train 005 3.125000e-02 8.055974e-01\n",
      "06/08 11:56:58 PM train_loss 0.036667, train_f1 0.802964\n",
      "06/08 11:56:58 PM epoch 81 lr 4.153449e-02\n",
      "06/08 11:56:58 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:56:59 PM train 005 3.645833e-02 7.792320e-01\n",
      "06/08 11:56:59 PM train_loss 0.046667, train_f1 0.793811\n",
      "06/08 11:56:59 PM epoch 82 lr 4.133725e-02\n",
      "06/08 11:56:59 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:59 PM train 005 3.645833e-02 7.404335e-01\n",
      "06/08 11:56:59 PM train_loss 0.036667, train_f1 0.763586\n",
      "06/08 11:56:59 PM epoch 83 lr 4.113821e-02\n",
      "06/08 11:56:59 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:56:59 PM train 005 4.687500e-02 6.820037e-01\n",
      "06/08 11:57:00 PM train_loss 0.040000, train_f1 0.762592\n",
      "06/08 11:57:00 PM epoch 84 lr 4.093741e-02\n",
      "06/08 11:57:00 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:00 PM train 005 3.645833e-02 7.792320e-01\n",
      "06/08 11:57:00 PM train_loss 0.043333, train_f1 0.804237\n",
      "06/08 11:57:00 PM epoch 85 lr 4.073486e-02\n",
      "06/08 11:57:00 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:00 PM train 005 4.166667e-02 7.223544e-01\n",
      "06/08 11:57:00 PM train_loss 0.043333, train_f1 0.768198\n",
      "06/08 11:57:00 PM epoch 86 lr 4.053059e-02\n",
      "06/08 11:57:01 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:01 PM train 005 4.687500e-02 6.876982e-01\n",
      "06/08 11:57:01 PM train_loss 0.050000, train_f1 0.718506\n",
      "06/08 11:57:01 PM epoch 87 lr 4.032461e-02\n",
      "06/08 11:57:01 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:01 PM train 005 3.645833e-02 7.209413e-01\n",
      "06/08 11:57:01 PM train_loss 0.036667, train_f1 0.777720\n",
      "06/08 11:57:01 PM epoch 88 lr 4.011695e-02\n",
      "06/08 11:57:01 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:02 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:02 PM train_loss 0.033333, train_f1 0.853702\n",
      "06/08 11:57:02 PM epoch 89 lr 3.990764e-02\n",
      "06/08 11:57:02 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:02 PM train 005 3.645833e-02 7.959457e-01\n",
      "06/08 11:57:02 PM train_loss 0.040000, train_f1 0.820274\n",
      "06/08 11:57:02 PM epoch 90 lr 3.969669e-02\n",
      "06/08 11:57:02 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:02 PM train 005 2.604167e-02 7.986711e-01\n",
      "06/08 11:57:03 PM train_loss 0.036667, train_f1 0.779086\n",
      "06/08 11:57:03 PM epoch 91 lr 3.948413e-02\n",
      "06/08 11:57:03 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:03 PM train 005 2.604167e-02 8.236765e-01\n",
      "06/08 11:57:03 PM train_loss 0.043333, train_f1 0.826201\n",
      "06/08 11:57:03 PM epoch 92 lr 3.926999e-02\n",
      "06/08 11:57:03 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:03 PM train 005 2.604167e-02 7.931642e-01\n",
      "06/08 11:57:03 PM train_loss 0.036667, train_f1 0.794104\n",
      "06/08 11:57:03 PM epoch 93 lr 3.905427e-02\n",
      "06/08 11:57:03 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:04 PM train 005 2.083333e-02 8.112433e-01\n",
      "06/08 11:57:04 PM train_loss 0.026667, train_f1 0.835513\n",
      "06/08 11:57:04 PM epoch 94 lr 3.883702e-02\n",
      "06/08 11:57:04 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:04 PM train 005 2.604167e-02 7.681643e-01\n",
      "06/08 11:57:04 PM train_loss 0.036667, train_f1 0.779498\n",
      "06/08 11:57:04 PM epoch 95 lr 3.861825e-02\n",
      "06/08 11:57:04 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:05 PM train 005 2.604167e-02 8.126117e-01\n",
      "06/08 11:57:05 PM train_loss 0.033333, train_f1 0.830940\n",
      "06/08 11:57:05 PM epoch 96 lr 3.839799e-02\n",
      "06/08 11:57:05 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:05 PM train 005 3.645833e-02 7.931672e-01\n",
      "06/08 11:57:05 PM train_loss 0.043333, train_f1 0.795500\n",
      "06/08 11:57:05 PM epoch 97 lr 3.817626e-02\n",
      "06/08 11:57:05 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:05 PM train 005 5.208333e-02 6.418406e-01\n",
      "06/08 11:57:06 PM train_loss 0.050000, train_f1 0.695037\n",
      "06/08 11:57:06 PM epoch 98 lr 3.795309e-02\n",
      "06/08 11:57:06 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:06 PM train 005 2.604167e-02 8.126117e-01\n",
      "06/08 11:57:06 PM train_loss 0.030000, train_f1 0.836389\n",
      "06/08 11:57:06 PM epoch 99 lr 3.772849e-02\n",
      "06/08 11:57:06 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:06 PM train 005 2.083333e-02 8.389770e-01\n",
      "06/08 11:57:06 PM train_loss 0.033333, train_f1 0.820302\n",
      "06/08 11:57:06 PM epoch 100 lr 3.750250e-02\n",
      "06/08 11:57:06 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:07 PM train 005 3.645833e-02 7.209413e-01\n",
      "06/08 11:57:07 PM train_loss 0.040000, train_f1 0.752804\n",
      "06/08 11:57:07 PM epoch 101 lr 3.727514e-02\n",
      "06/08 11:57:07 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:07 PM train 005 2.083333e-02 8.112433e-01\n",
      "06/08 11:57:07 PM train_loss 0.033333, train_f1 0.818820\n",
      "06/08 11:57:07 PM epoch 102 lr 3.704643e-02\n",
      "06/08 11:57:07 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:08 PM train 005 4.687500e-02 6.682106e-01\n",
      "06/08 11:57:08 PM train_loss 0.046667, train_f1 0.740301\n",
      "06/08 11:57:08 PM epoch 103 lr 3.681641e-02\n",
      "06/08 11:57:08 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:08 PM train 005 2.604167e-02 8.319627e-01\n",
      "06/08 11:57:08 PM train_loss 0.026667, train_f1 0.830507\n",
      "06/08 11:57:08 PM epoch 104 lr 3.658508e-02\n",
      "06/08 11:57:08 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:08 PM train 005 2.083333e-02 8.389770e-01\n",
      "06/08 11:57:08 PM train_loss 0.033333, train_f1 0.771571\n",
      "06/08 11:57:08 PM epoch 105 lr 3.635249e-02\n",
      "06/08 11:57:09 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:09 PM train 005 2.083333e-02 8.528204e-01\n",
      "06/08 11:57:09 PM train_loss 0.026667, train_f1 0.838109\n",
      "06/08 11:57:09 PM epoch 106 lr 3.611866e-02\n",
      "06/08 11:57:09 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:09 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:57:09 PM train_loss 0.033333, train_f1 0.850700\n",
      "06/08 11:57:09 PM epoch 107 lr 3.588360e-02\n",
      "06/08 11:57:09 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:10 PM train 005 3.125000e-02 8.250896e-01\n",
      "06/08 11:57:10 PM train_loss 0.033333, train_f1 0.844375\n",
      "06/08 11:57:10 PM epoch 108 lr 3.564735e-02\n",
      "06/08 11:57:10 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:10 PM train 005 2.604167e-02 8.208979e-01\n",
      "06/08 11:57:10 PM train_loss 0.036667, train_f1 0.808441\n",
      "06/08 11:57:10 PM epoch 109 lr 3.540994e-02\n",
      "06/08 11:57:10 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:10 PM train 005 2.083333e-02 8.500418e-01\n",
      "06/08 11:57:11 PM train_loss 0.026667, train_f1 0.849267\n",
      "06/08 11:57:11 PM epoch 110 lr 3.517138e-02\n",
      "06/08 11:57:11 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:11 PM train 005 3.645833e-02 6.710780e-01\n",
      "06/08 11:57:11 PM train_loss 0.046667, train_f1 0.706937\n",
      "06/08 11:57:11 PM epoch 111 lr 3.493171e-02\n",
      "06/08 11:57:11 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:11 PM train 005 2.083333e-02 7.834648e-01\n",
      "06/08 11:57:11 PM train_loss 0.026667, train_f1 0.798566\n",
      "06/08 11:57:11 PM epoch 112 lr 3.469095e-02\n",
      "06/08 11:57:11 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:12 PM train 005 3.125000e-02 8.084677e-01\n",
      "06/08 11:57:12 PM train_loss 0.036667, train_f1 0.806225\n",
      "06/08 11:57:12 PM epoch 113 lr 3.444913e-02\n",
      "06/08 11:57:12 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:12 PM train 005 2.083333e-02 8.389770e-01\n",
      "06/08 11:57:12 PM train_loss 0.023333, train_f1 0.834996\n",
      "06/08 11:57:12 PM epoch 114 lr 3.420627e-02\n",
      "06/08 11:57:12 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:12 PM train 005 3.645833e-02 7.930251e-01\n",
      "06/08 11:57:13 PM train_loss 0.043333, train_f1 0.807160\n",
      "06/08 11:57:13 PM epoch 115 lr 3.396241e-02\n",
      "06/08 11:57:13 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:13 PM train 005 2.604167e-02 8.236765e-01\n",
      "06/08 11:57:13 PM train_loss 0.030000, train_f1 0.847814\n",
      "06/08 11:57:13 PM epoch 116 lr 3.371756e-02\n",
      "06/08 11:57:13 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:13 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:57:13 PM train_loss 0.016667, train_f1 0.890050\n",
      "06/08 11:57:13 PM epoch 117 lr 3.347175e-02\n",
      "06/08 11:57:14 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:14 PM train 005 2.083333e-02 8.555487e-01\n",
      "06/08 11:57:14 PM train_loss 0.023333, train_f1 0.845602\n",
      "06/08 11:57:14 PM epoch 118 lr 3.322502e-02\n",
      "06/08 11:57:14 PM train 000 6.250000e-02 4.838710e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:57:14 PM train 005 5.208333e-02 6.946237e-01\n",
      "06/08 11:57:14 PM train_loss 0.046667, train_f1 0.734267\n",
      "06/08 11:57:14 PM epoch 119 lr 3.297739e-02\n",
      "06/08 11:57:14 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:15 PM train 005 3.645833e-02 6.571429e-01\n",
      "06/08 11:57:15 PM train_loss 0.046667, train_f1 0.722560\n",
      "06/08 11:57:15 PM epoch 120 lr 3.272888e-02\n",
      "06/08 11:57:15 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:15 PM train 005 3.645833e-02 7.126048e-01\n",
      "06/08 11:57:15 PM train_loss 0.043333, train_f1 0.755691\n",
      "06/08 11:57:15 PM epoch 121 lr 3.247952e-02\n",
      "06/08 11:57:15 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:15 PM train 005 5.208333e-02 6.668452e-01\n",
      "06/08 11:57:16 PM train_loss 0.053333, train_f1 0.731382\n",
      "06/08 11:57:16 PM epoch 122 lr 3.222935e-02\n",
      "06/08 11:57:16 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:16 PM train 005 2.604167e-02 8.514550e-01\n",
      "06/08 11:57:16 PM train_loss 0.033333, train_f1 0.855800\n",
      "06/08 11:57:16 PM epoch 123 lr 3.197838e-02\n",
      "06/08 11:57:16 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:16 PM train 005 4.166667e-02 7.778666e-01\n",
      "06/08 11:57:16 PM train_loss 0.043333, train_f1 0.807885\n",
      "06/08 11:57:16 PM epoch 124 lr 3.172665e-02\n",
      "06/08 11:57:16 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:17 PM train 005 6.770833e-02 5.988093e-01\n",
      "06/08 11:57:17 PM train_loss 0.063333, train_f1 0.646628\n",
      "06/08 11:57:17 PM epoch 125 lr 3.147418e-02\n",
      "06/08 11:57:17 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:17 PM train 005 2.083333e-02 8.500418e-01\n",
      "06/08 11:57:17 PM train_loss 0.030000, train_f1 0.854077\n",
      "06/08 11:57:17 PM epoch 126 lr 3.122100e-02\n",
      "06/08 11:57:17 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:17 PM train 005 3.125000e-02 7.500852e-01\n",
      "06/08 11:57:18 PM train_loss 0.043333, train_f1 0.762479\n",
      "06/08 11:57:18 PM epoch 127 lr 3.096714e-02\n",
      "06/08 11:57:18 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:18 PM train 005 3.645833e-02 7.959457e-01\n",
      "06/08 11:57:18 PM train_loss 0.043333, train_f1 0.814007\n",
      "06/08 11:57:18 PM epoch 128 lr 3.071263e-02\n",
      "06/08 11:57:18 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:18 PM train 005 2.083333e-02 9.055540e-01\n",
      "06/08 11:57:18 PM train_loss 0.030000, train_f1 0.870956\n",
      "06/08 11:57:18 PM epoch 129 lr 3.045749e-02\n",
      "06/08 11:57:18 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:19 PM train 005 3.645833e-02 7.487198e-01\n",
      "06/08 11:57:19 PM train_loss 0.043333, train_f1 0.769650\n",
      "06/08 11:57:19 PM epoch 130 lr 3.020175e-02\n",
      "06/08 11:57:19 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:19 PM train 005 5.208333e-02 7.195788e-01\n",
      "06/08 11:57:19 PM train_loss 0.053333, train_f1 0.770437\n",
      "06/08 11:57:19 PM epoch 131 lr 2.994545e-02\n",
      "06/08 11:57:19 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:19 PM train 005 4.687500e-02 7.515013e-01\n",
      "06/08 11:57:20 PM train_loss 0.046667, train_f1 0.791011\n",
      "06/08 11:57:20 PM epoch 132 lr 2.968860e-02\n",
      "06/08 11:57:20 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:20 PM train 005 5.208333e-02 6.945789e-01\n",
      "06/08 11:57:20 PM train_loss 0.053333, train_f1 0.731010\n",
      "06/08 11:57:20 PM epoch 133 lr 2.943123e-02\n",
      "06/08 11:57:20 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:20 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:20 PM train_loss 0.033333, train_f1 0.844284\n",
      "06/08 11:57:20 PM epoch 134 lr 2.917338e-02\n",
      "06/08 11:57:20 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:21 PM train 005 5.208333e-02 6.502233e-01\n",
      "06/08 11:57:21 PM train_loss 0.053333, train_f1 0.721671\n",
      "06/08 11:57:21 PM epoch 135 lr 2.891508e-02\n",
      "06/08 11:57:21 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:21 PM train 005 3.125000e-02 7.945326e-01\n",
      "06/08 11:57:21 PM train_loss 0.043333, train_f1 0.810489\n",
      "06/08 11:57:21 PM epoch 136 lr 2.865635e-02\n",
      "06/08 11:57:21 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:21 PM train 005 2.604167e-02 8.791887e-01\n",
      "06/08 11:57:22 PM train_loss 0.030000, train_f1 0.881594\n",
      "06/08 11:57:22 PM epoch 137 lr 2.839721e-02\n",
      "06/08 11:57:22 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:22 PM train 005 4.166667e-02 6.640637e-01\n",
      "06/08 11:57:22 PM train_loss 0.043333, train_f1 0.716403\n",
      "06/08 11:57:22 PM epoch 138 lr 2.813770e-02\n",
      "06/08 11:57:22 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:22 PM train 005 4.166667e-02 7.473544e-01\n",
      "06/08 11:57:22 PM train_loss 0.046667, train_f1 0.748839\n",
      "06/08 11:57:23 PM epoch 139 lr 2.787785e-02\n",
      "06/08 11:57:23 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:23 PM train 005 3.645833e-02 7.626549e-01\n",
      "06/08 11:57:23 PM train_loss 0.033333, train_f1 0.812759\n",
      "06/08 11:57:23 PM epoch 140 lr 2.761769e-02\n",
      "06/08 11:57:23 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:23 PM train 005 3.125000e-02 7.500852e-01\n",
      "06/08 11:57:23 PM train_loss 0.036667, train_f1 0.790105\n",
      "06/08 11:57:23 PM epoch 141 lr 2.735724e-02\n",
      "06/08 11:57:23 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:24 PM train 005 3.125000e-02 8.055974e-01\n",
      "06/08 11:57:24 PM train_loss 0.043333, train_f1 0.805087\n",
      "06/08 11:57:24 PM epoch 142 lr 2.709653e-02\n",
      "06/08 11:57:24 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:24 PM train 005 3.645833e-02 8.347442e-01\n",
      "06/08 11:57:24 PM train_loss 0.036667, train_f1 0.778107\n",
      "06/08 11:57:24 PM epoch 143 lr 2.683559e-02\n",
      "06/08 11:57:24 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:24 PM train 005 2.083333e-02 9.193974e-01\n",
      "06/08 11:57:25 PM train_loss 0.026667, train_f1 0.904732\n",
      "06/08 11:57:25 PM epoch 144 lr 2.657445e-02\n",
      "06/08 11:57:25 PM train 000 9.375000e-02 4.754098e-01\n",
      "06/08 11:57:25 PM train 005 4.166667e-02 8.056003e-01\n",
      "06/08 11:57:25 PM train_loss 0.040000, train_f1 0.787514\n",
      "06/08 11:57:25 PM epoch 145 lr 2.631314e-02\n",
      "06/08 11:57:25 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:25 PM train 005 2.604167e-02 8.819672e-01\n",
      "06/08 11:57:25 PM train_loss 0.030000, train_f1 0.872295\n",
      "06/08 11:57:25 PM epoch 146 lr 2.605168e-02\n",
      "06/08 11:57:25 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:26 PM train 005 4.166667e-02 7.778666e-01\n",
      "06/08 11:57:26 PM train_loss 0.046667, train_f1 0.727914\n",
      "06/08 11:57:26 PM epoch 147 lr 2.579011e-02\n",
      "06/08 11:57:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:26 PM train 005 3.125000e-02 8.223111e-01\n",
      "06/08 11:57:26 PM train_loss 0.030000, train_f1 0.859470\n",
      "06/08 11:57:26 PM epoch 148 lr 2.552846e-02\n",
      "06/08 11:57:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:26 PM train 005 3.125000e-02 8.333759e-01\n",
      "06/08 11:57:27 PM train_loss 0.033333, train_f1 0.849678\n",
      "06/08 11:57:27 PM epoch 149 lr 2.526674e-02\n",
      "06/08 11:57:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:27 PM train 005 2.604167e-02 9.068251e-01\n",
      "06/08 11:57:27 PM train_loss 0.033333, train_f1 0.871770\n",
      "06/08 11:57:27 PM epoch 150 lr 2.500500e-02\n",
      "06/08 11:57:27 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:27 PM train 005 3.645833e-02 7.653887e-01\n",
      "06/08 11:57:27 PM train_loss 0.040000, train_f1 0.785297\n",
      "06/08 11:57:27 PM epoch 151 lr 2.474326e-02\n",
      "06/08 11:57:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:28 PM train 005 4.166667e-02 7.666598e-01\n",
      "06/08 11:57:28 PM train_loss 0.036667, train_f1 0.823854\n",
      "06/08 11:57:28 PM epoch 152 lr 2.448154e-02\n",
      "06/08 11:57:28 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:28 PM train 005 4.687500e-02 7.348794e-01\n",
      "06/08 11:57:28 PM train_loss 0.040000, train_f1 0.794983\n",
      "06/08 11:57:28 PM epoch 153 lr 2.421989e-02\n",
      "06/08 11:57:28 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:28 PM train 005 3.125000e-02 7.640203e-01\n",
      "06/08 11:57:29 PM train_loss 0.040000, train_f1 0.794501\n",
      "06/08 11:57:29 PM epoch 154 lr 2.395832e-02\n",
      "06/08 11:57:29 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:29 PM train 005 3.645833e-02 7.931672e-01\n",
      "06/08 11:57:29 PM train_loss 0.040000, train_f1 0.817677\n",
      "06/08 11:57:29 PM epoch 155 lr 2.369686e-02\n",
      "06/08 11:57:29 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:29 PM train 005 5.208333e-02 7.195788e-01\n",
      "06/08 11:57:29 PM train_loss 0.050000, train_f1 0.773177\n",
      "06/08 11:57:29 PM epoch 156 lr 2.343555e-02\n",
      "06/08 11:57:30 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:30 PM train 005 3.645833e-02 8.209009e-01\n",
      "06/08 11:57:30 PM train_loss 0.043333, train_f1 0.830905\n",
      "06/08 11:57:30 PM epoch 157 lr 2.317441e-02\n",
      "06/08 11:57:30 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:30 PM train 005 2.604167e-02 8.374696e-01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:57:30 PM train_loss 0.030000, train_f1 0.779851\n",
      "06/08 11:57:30 PM epoch 158 lr 2.291347e-02\n",
      "06/08 11:57:30 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:31 PM train 005 3.125000e-02 8.055974e-01\n",
      "06/08 11:57:31 PM train_loss 0.036667, train_f1 0.825632\n",
      "06/08 11:57:31 PM epoch 159 lr 2.265276e-02\n",
      "06/08 11:57:31 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:31 PM train 005 2.604167e-02 8.208979e-01\n",
      "06/08 11:57:31 PM train_loss 0.036667, train_f1 0.762625\n",
      "06/08 11:57:31 PM epoch 160 lr 2.239231e-02\n",
      "06/08 11:57:31 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:31 PM train 005 4.687500e-02 7.542304e-01\n",
      "06/08 11:57:32 PM train_loss 0.050000, train_f1 0.777959\n",
      "06/08 11:57:32 PM epoch 161 lr 2.213215e-02\n",
      "06/08 11:57:32 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:32 PM train 005 3.645833e-02 8.042320e-01\n",
      "06/08 11:57:32 PM train_loss 0.040000, train_f1 0.802090\n",
      "06/08 11:57:32 PM epoch 162 lr 2.187230e-02\n",
      "06/08 11:57:32 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:32 PM train 005 4.166667e-02 7.778666e-01\n",
      "06/08 11:57:32 PM train_loss 0.040000, train_f1 0.822980\n",
      "06/08 11:57:32 PM epoch 163 lr 2.161279e-02\n",
      "06/08 11:57:32 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:33 PM train 005 3.125000e-02 8.638881e-01\n",
      "06/08 11:57:33 PM train_loss 0.030000, train_f1 0.877548\n",
      "06/08 11:57:33 PM epoch 164 lr 2.135365e-02\n",
      "06/08 11:57:33 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:33 PM train 005 4.687500e-02 6.626983e-01\n",
      "06/08 11:57:33 PM train_loss 0.046667, train_f1 0.662251\n",
      "06/08 11:57:33 PM epoch 165 lr 2.109492e-02\n",
      "06/08 11:57:33 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:33 PM train 005 3.645833e-02 8.042320e-01\n",
      "06/08 11:57:34 PM train_loss 0.050000, train_f1 0.723044\n",
      "06/08 11:57:34 PM epoch 166 lr 2.083662e-02\n",
      "06/08 11:57:34 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:34 PM train 005 2.604167e-02 8.930320e-01\n",
      "06/08 11:57:34 PM train_loss 0.033333, train_f1 0.881591\n",
      "06/08 11:57:34 PM epoch 167 lr 2.057877e-02\n",
      "06/08 11:57:34 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:34 PM train 005 3.125000e-02 7.640203e-01\n",
      "06/08 11:57:34 PM train_loss 0.030000, train_f1 0.817679\n",
      "06/08 11:57:34 PM epoch 168 lr 2.032140e-02\n",
      "06/08 11:57:34 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:35 PM train 005 3.645833e-02 8.042320e-01\n",
      "06/08 11:57:35 PM train_loss 0.040000, train_f1 0.786143\n",
      "06/08 11:57:35 PM epoch 169 lr 2.006455e-02\n",
      "06/08 11:57:35 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:35 PM train 005 3.125000e-02 8.333759e-01\n",
      "06/08 11:57:35 PM train_loss 0.030000, train_f1 0.866552\n",
      "06/08 11:57:35 PM epoch 170 lr 1.980825e-02\n",
      "06/08 11:57:35 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:35 PM train 005 3.125000e-02 8.333759e-01\n",
      "06/08 11:57:36 PM train_loss 0.036667, train_f1 0.824762\n",
      "06/08 11:57:36 PM epoch 171 lr 1.955251e-02\n",
      "06/08 11:57:36 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:36 PM train 005 3.645833e-02 7.375129e-01\n",
      "06/08 11:57:36 PM train_loss 0.033333, train_f1 0.796668\n",
      "06/08 11:57:36 PM epoch 172 lr 1.929737e-02\n",
      "06/08 11:57:36 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:36 PM train 005 4.166667e-02 8.194437e-01\n",
      "06/08 11:57:36 PM train_loss 0.046667, train_f1 0.807801\n",
      "06/08 11:57:36 PM epoch 173 lr 1.904286e-02\n",
      "06/08 11:57:37 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:37 PM train 005 3.645833e-02 7.930251e-01\n",
      "06/08 11:57:37 PM train_loss 0.046667, train_f1 0.794445\n",
      "06/08 11:57:37 PM epoch 174 lr 1.878900e-02\n",
      "06/08 11:57:37 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:37 PM train 005 5.208333e-02 6.779570e-01\n",
      "06/08 11:57:37 PM train_loss 0.050000, train_f1 0.743943\n",
      "06/08 11:57:37 PM epoch 175 lr 1.853582e-02\n",
      "06/08 11:57:37 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:38 PM train 005 4.687500e-02 6.932105e-01\n",
      "06/08 11:57:38 PM train_loss 0.046667, train_f1 0.753705\n",
      "06/08 11:57:38 PM epoch 176 lr 1.828335e-02\n",
      "06/08 11:57:38 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:38 PM train 005 4.687500e-02 7.764564e-01\n",
      "06/08 11:57:38 PM train_loss 0.040000, train_f1 0.821592\n",
      "06/08 11:57:38 PM epoch 177 lr 1.803162e-02\n",
      "06/08 11:57:38 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:38 PM train 005 3.645833e-02 7.792320e-01\n",
      "06/08 11:57:39 PM train_loss 0.036667, train_f1 0.773235\n",
      "06/08 11:57:39 PM epoch 178 lr 1.778065e-02\n",
      "06/08 11:57:39 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:39 PM train 005 3.125000e-02 8.195325e-01\n",
      "06/08 11:57:39 PM train_loss 0.033333, train_f1 0.843414\n",
      "06/08 11:57:39 PM epoch 179 lr 1.753048e-02\n",
      "06/08 11:57:39 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:39 PM train 005 3.645833e-02 8.347442e-01\n",
      "06/08 11:57:39 PM train_loss 0.036667, train_f1 0.826541\n",
      "06/08 11:57:39 PM epoch 180 lr 1.728112e-02\n",
      "06/08 11:57:39 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:40 PM train 005 3.645833e-02 7.930251e-01\n",
      "06/08 11:57:40 PM train_loss 0.033333, train_f1 0.840727\n",
      "06/08 11:57:40 PM epoch 181 lr 1.703261e-02\n",
      "06/08 11:57:40 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:40 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:40 PM train_loss 0.036667, train_f1 0.839763\n",
      "06/08 11:57:40 PM epoch 182 lr 1.678498e-02\n",
      "06/08 11:57:40 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:40 PM train 005 2.083333e-02 9.055540e-01\n",
      "06/08 11:57:41 PM train_loss 0.023333, train_f1 0.908260\n",
      "06/08 11:57:41 PM epoch 183 lr 1.653825e-02\n",
      "06/08 11:57:41 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:41 PM train 005 4.166667e-02 6.945759e-01\n",
      "06/08 11:57:41 PM train_loss 0.040000, train_f1 0.762921\n",
      "06/08 11:57:41 PM epoch 184 lr 1.629244e-02\n",
      "06/08 11:57:41 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:41 PM train 005 3.645833e-02 8.070105e-01\n",
      "06/08 11:57:41 PM train_loss 0.040000, train_f1 0.829133\n",
      "06/08 11:57:41 PM epoch 185 lr 1.604759e-02\n",
      "06/08 11:57:41 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:42 PM train 005 2.604167e-02 8.764101e-01\n",
      "06/08 11:57:42 PM train_loss 0.030000, train_f1 0.868739\n",
      "06/08 11:57:42 PM epoch 186 lr 1.580373e-02\n",
      "06/08 11:57:42 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:42 PM train 005 2.604167e-02 8.486764e-01\n",
      "06/08 11:57:42 PM train_loss 0.030000, train_f1 0.835457\n",
      "06/08 11:57:42 PM epoch 187 lr 1.556087e-02\n",
      "06/08 11:57:42 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:42 PM train 005 2.083333e-02 8.528204e-01\n",
      "06/08 11:57:43 PM train_loss 0.030000, train_f1 0.855855\n",
      "06/08 11:57:43 PM epoch 188 lr 1.531905e-02\n",
      "06/08 11:57:43 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:43 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:43 PM train_loss 0.026667, train_f1 0.858894\n",
      "06/08 11:57:43 PM epoch 189 lr 1.507829e-02\n",
      "06/08 11:57:43 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:43 PM train 005 3.125000e-02 8.361042e-01\n",
      "06/08 11:57:43 PM train_loss 0.036667, train_f1 0.773231\n",
      "06/08 11:57:43 PM epoch 190 lr 1.483862e-02\n",
      "06/08 11:57:43 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:44 PM train 005 3.125000e-02 8.333759e-01\n",
      "06/08 11:57:44 PM train_loss 0.030000, train_f1 0.862066\n",
      "06/08 11:57:44 PM epoch 191 lr 1.460006e-02\n",
      "06/08 11:57:44 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:44 PM train 005 4.166667e-02 6.945759e-01\n",
      "06/08 11:57:44 PM train_loss 0.043333, train_f1 0.754579\n",
      "06/08 11:57:44 PM epoch 192 lr 1.436265e-02\n",
      "06/08 11:57:44 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:45 PM train 005 4.687500e-02 7.348794e-01\n",
      "06/08 11:57:45 PM train_loss 0.046667, train_f1 0.774598\n",
      "06/08 11:57:45 PM epoch 193 lr 1.412640e-02\n",
      "06/08 11:57:45 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:45 PM train 005 3.645833e-02 8.347442e-01\n",
      "06/08 11:57:45 PM train_loss 0.030000, train_f1 0.868689\n",
      "06/08 11:57:45 PM epoch 194 lr 1.389134e-02\n",
      "06/08 11:57:45 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:45 PM train 005 3.645833e-02 8.208036e-01\n",
      "06/08 11:57:45 PM train_loss 0.036667, train_f1 0.817619\n",
      "06/08 11:57:45 PM epoch 195 lr 1.365751e-02\n",
      "06/08 11:57:46 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:46 PM train 005 4.166667e-02 7.361475e-01\n",
      "06/08 11:57:46 PM train_loss 0.036667, train_f1 0.799840\n",
      "06/08 11:57:46 PM epoch 196 lr 1.342492e-02\n",
      "06/08 11:57:46 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:46 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:57:46 PM train_loss 0.023333, train_f1 0.873990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:57:46 PM epoch 197 lr 1.319359e-02\n",
      "06/08 11:57:46 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:47 PM train 005 3.125000e-02 8.472662e-01\n",
      "06/08 11:57:47 PM train_loss 0.036667, train_f1 0.842042\n",
      "06/08 11:57:47 PM epoch 198 lr 1.296357e-02\n",
      "06/08 11:57:47 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:47 PM train 005 2.083333e-02 8.667555e-01\n",
      "06/08 11:57:47 PM train_loss 0.016667, train_f1 0.907828\n",
      "06/08 11:57:47 PM epoch 199 lr 1.273486e-02\n",
      "06/08 11:57:47 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:47 PM train 005 2.604167e-02 8.514550e-01\n",
      "06/08 11:57:48 PM train_loss 0.030000, train_f1 0.816861\n",
      "06/08 11:57:48 PM epoch 200 lr 1.250750e-02\n",
      "06/08 11:57:48 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:48 PM train 005 2.083333e-02 9.193974e-01\n",
      "06/08 11:57:48 PM train_loss 0.023333, train_f1 0.921606\n",
      "06/08 11:57:48 PM epoch 201 lr 1.228151e-02\n",
      "06/08 11:57:48 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:48 PM train 005 3.125000e-02 8.055974e-01\n",
      "06/08 11:57:48 PM train_loss 0.046667, train_f1 0.801494\n",
      "06/08 11:57:48 PM epoch 202 lr 1.205691e-02\n",
      "06/08 11:57:48 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:49 PM train 005 4.687500e-02 7.764564e-01\n",
      "06/08 11:57:49 PM train_loss 0.040000, train_f1 0.794983\n",
      "06/08 11:57:49 PM epoch 203 lr 1.183374e-02\n",
      "06/08 11:57:49 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:49 PM train 005 3.125000e-02 7.667989e-01\n",
      "06/08 11:57:49 PM train_loss 0.033333, train_f1 0.809144\n",
      "06/08 11:57:49 PM epoch 204 lr 1.161201e-02\n",
      "06/08 11:57:49 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:49 PM train 005 2.083333e-02 9.193974e-01\n",
      "06/08 11:57:50 PM train_loss 0.023333, train_f1 0.913074\n",
      "06/08 11:57:50 PM epoch 205 lr 1.139175e-02\n",
      "06/08 11:57:50 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:50 PM train 005 2.604167e-02 8.208979e-01\n",
      "06/08 11:57:50 PM train_loss 0.030000, train_f1 0.844288\n",
      "06/08 11:57:50 PM epoch 206 lr 1.117298e-02\n",
      "06/08 11:57:50 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:50 PM train 005 3.125000e-02 8.638881e-01\n",
      "06/08 11:57:50 PM train_loss 0.030000, train_f1 0.877548\n",
      "06/08 11:57:50 PM epoch 207 lr 1.095573e-02\n",
      "06/08 11:57:50 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:51 PM train 005 3.645833e-02 7.514481e-01\n",
      "06/08 11:57:51 PM train_loss 0.040000, train_f1 0.793573\n",
      "06/08 11:57:51 PM epoch 208 lr 1.074001e-02\n",
      "06/08 11:57:51 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:51 PM train 005 3.125000e-02 8.638881e-01\n",
      "06/08 11:57:51 PM train_loss 0.040000, train_f1 0.828234\n",
      "06/08 11:57:51 PM epoch 209 lr 1.052587e-02\n",
      "06/08 11:57:51 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:51 PM train 005 3.125000e-02 8.472662e-01\n",
      "06/08 11:57:52 PM train_loss 0.033333, train_f1 0.786121\n",
      "06/08 11:57:52 PM epoch 210 lr 1.031331e-02\n",
      "06/08 11:57:52 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:52 PM train 005 4.687500e-02 6.932105e-01\n",
      "06/08 11:57:52 PM train_loss 0.043333, train_f1 0.762568\n",
      "06/08 11:57:52 PM epoch 211 lr 1.010236e-02\n",
      "06/08 11:57:52 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:52 PM train 005 2.604167e-02 8.902535e-01\n",
      "06/08 11:57:52 PM train_loss 0.023333, train_f1 0.904215\n",
      "06/08 11:57:52 PM epoch 212 lr 9.893045e-03\n",
      "06/08 11:57:53 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:53 PM train 005 4.687500e-02 7.347373e-01\n",
      "06/08 11:57:53 PM train_loss 0.040000, train_f1 0.794892\n",
      "06/08 11:57:53 PM epoch 213 lr 9.685388e-03\n",
      "06/08 11:57:53 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:53 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:53 PM train_loss 0.036667, train_f1 0.839763\n",
      "06/08 11:57:53 PM epoch 214 lr 9.479411e-03\n",
      "06/08 11:57:53 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:54 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:54 PM train_loss 0.036667, train_f1 0.829486\n",
      "06/08 11:57:54 PM epoch 215 lr 9.275137e-03\n",
      "06/08 11:57:54 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:54 PM train 005 1.562500e-02 9.346979e-01\n",
      "06/08 11:57:54 PM train_loss 0.020000, train_f1 0.926913\n",
      "06/08 11:57:54 PM epoch 216 lr 9.072587e-03\n",
      "06/08 11:57:54 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:54 PM train 005 3.645833e-02 8.070105e-01\n",
      "06/08 11:57:55 PM train_loss 0.033333, train_f1 0.845193\n",
      "06/08 11:57:55 PM epoch 217 lr 8.871785e-03\n",
      "06/08 11:57:55 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:55 PM train 005 3.645833e-02 7.487198e-01\n",
      "06/08 11:57:55 PM train_loss 0.033333, train_f1 0.807887\n",
      "06/08 11:57:55 PM epoch 218 lr 8.672752e-03\n",
      "06/08 11:57:55 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:55 PM train 005 3.125000e-02 8.221690e-01\n",
      "06/08 11:57:55 PM train_loss 0.033333, train_f1 0.845102\n",
      "06/08 11:57:55 PM epoch 219 lr 8.475510e-03\n",
      "06/08 11:57:55 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:56 PM train 005 2.604167e-02 8.791887e-01\n",
      "06/08 11:57:56 PM train_loss 0.030000, train_f1 0.881594\n",
      "06/08 11:57:56 PM epoch 220 lr 8.280080e-03\n",
      "06/08 11:57:56 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:57:56 PM train 005 4.166667e-02 7.361475e-01\n",
      "06/08 11:57:56 PM train_loss 0.036667, train_f1 0.799840\n",
      "06/08 11:57:56 PM epoch 221 lr 8.086485e-03\n",
      "06/08 11:57:56 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:57 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:57:57 PM train_loss 0.026667, train_f1 0.858894\n",
      "06/08 11:57:57 PM epoch 222 lr 7.894745e-03\n",
      "06/08 11:57:57 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:57 PM train 005 3.125000e-02 8.361042e-01\n",
      "06/08 11:57:57 PM train_loss 0.030000, train_f1 0.863813\n",
      "06/08 11:57:57 PM epoch 223 lr 7.704881e-03\n",
      "06/08 11:57:57 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:57 PM train 005 3.645833e-02 7.487198e-01\n",
      "06/08 11:57:57 PM train_loss 0.040000, train_f1 0.750615\n",
      "06/08 11:57:57 PM epoch 224 lr 7.516915e-03\n",
      "06/08 11:57:58 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:58 PM train 005 3.125000e-02 8.221690e-01\n",
      "06/08 11:57:58 PM train_loss 0.033333, train_f1 0.842506\n",
      "06/08 11:57:58 PM epoch 225 lr 7.330866e-03\n",
      "06/08 11:57:58 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:57:58 PM train 005 4.166667e-02 7.249407e-01\n",
      "06/08 11:57:58 PM train_loss 0.040000, train_f1 0.782876\n",
      "06/08 11:57:58 PM epoch 226 lr 7.146755e-03\n",
      "06/08 11:57:58 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:59 PM train 005 3.645833e-02 8.347442e-01\n",
      "06/08 11:57:59 PM train_loss 0.030000, train_f1 0.868689\n",
      "06/08 11:57:59 PM epoch 227 lr 6.964603e-03\n",
      "06/08 11:57:59 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:59 PM train 005 3.125000e-02 8.055974e-01\n",
      "06/08 11:57:59 PM train_loss 0.033333, train_f1 0.840727\n",
      "06/08 11:57:59 PM epoch 228 lr 6.784429e-03\n",
      "06/08 11:57:59 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:57:59 PM train 005 3.125000e-02 8.221690e-01\n",
      "06/08 11:58:00 PM train_loss 0.030000, train_f1 0.850848\n",
      "06/08 11:58:00 PM epoch 229 lr 6.606253e-03\n",
      "06/08 11:58:00 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:00 PM train 005 2.604167e-02 9.068251e-01\n",
      "06/08 11:58:00 PM train_loss 0.026667, train_f1 0.909074\n",
      "06/08 11:58:00 PM epoch 230 lr 6.430095e-03\n",
      "06/08 11:58:00 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:00 PM train 005 3.125000e-02 8.638881e-01\n",
      "06/08 11:58:00 PM train_loss 0.033333, train_f1 0.871802\n",
      "06/08 11:58:00 PM epoch 231 lr 6.255974e-03\n",
      "06/08 11:58:00 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:58:01 PM train 005 3.645833e-02 7.487198e-01\n",
      "06/08 11:58:01 PM train_loss 0.043333, train_f1 0.774432\n",
      "06/08 11:58:01 PM epoch 232 lr 6.083909e-03\n",
      "06/08 11:58:01 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:01 PM train 005 2.604167e-02 8.374696e-01\n",
      "06/08 11:58:01 PM train_loss 0.033333, train_f1 0.827382\n",
      "06/08 11:58:01 PM epoch 233 lr 5.913918e-03\n",
      "06/08 11:58:01 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:58:01 PM train 005 3.645833e-02 7.487198e-01\n",
      "06/08 11:58:02 PM train_loss 0.040000, train_f1 0.770582\n",
      "06/08 11:58:02 PM epoch 234 lr 5.746021e-03\n",
      "06/08 11:58:02 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:02 PM train 005 3.645833e-02 7.487198e-01\n",
      "06/08 11:58:02 PM train_loss 0.036667, train_f1 0.771485\n",
      "06/08 11:58:02 PM epoch 235 lr 5.580237e-03\n",
      "06/08 11:58:02 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:02 PM train 005 4.687500e-02 6.847831e-01\n",
      "06/08 11:58:02 PM train_loss 0.043333, train_f1 0.730565\n",
      "06/08 11:58:02 PM epoch 236 lr 5.416582e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:58:02 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:03 PM train 005 4.166667e-02 7.916597e-01\n",
      "06/08 11:58:03 PM train_loss 0.033333, train_f1 0.841115\n",
      "06/08 11:58:03 PM epoch 237 lr 5.255075e-03\n",
      "06/08 11:58:03 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:03 PM train 005 3.125000e-02 8.333759e-01\n",
      "06/08 11:58:03 PM train_loss 0.033333, train_f1 0.825665\n",
      "06/08 11:58:03 PM epoch 238 lr 5.095735e-03\n",
      "06/08 11:58:03 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:04 PM train 005 3.645833e-02 7.375129e-01\n",
      "06/08 11:58:04 PM train_loss 0.040000, train_f1 0.763410\n",
      "06/08 11:58:04 PM epoch 239 lr 4.938577e-03\n",
      "06/08 11:58:04 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:04 PM train 005 3.645833e-02 8.347442e-01\n",
      "06/08 11:58:04 PM train_loss 0.036667, train_f1 0.826541\n",
      "06/08 11:58:04 PM epoch 240 lr 4.783620e-03\n",
      "06/08 11:58:04 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:58:04 PM train 005 5.208333e-02 6.806383e-01\n",
      "06/08 11:58:05 PM train_loss 0.046667, train_f1 0.760754\n",
      "06/08 11:58:05 PM epoch 241 lr 4.630881e-03\n",
      "06/08 11:58:05 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:05 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:58:05 PM train_loss 0.030000, train_f1 0.826539\n",
      "06/08 11:58:05 PM epoch 242 lr 4.480376e-03\n",
      "06/08 11:58:05 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:58:05 PM train 005 3.645833e-02 8.042320e-01\n",
      "06/08 11:58:05 PM train_loss 0.036667, train_f1 0.831026\n",
      "06/08 11:58:05 PM epoch 243 lr 4.332121e-03\n",
      "06/08 11:58:05 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:06 PM train 005 2.083333e-02 8.528204e-01\n",
      "06/08 11:58:06 PM train_loss 0.023333, train_f1 0.874511\n",
      "06/08 11:58:06 PM epoch 244 lr 4.186134e-03\n",
      "06/08 11:58:06 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:06 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:58:06 PM train_loss 0.030000, train_f1 0.853148\n",
      "06/08 11:58:06 PM epoch 245 lr 4.042429e-03\n",
      "06/08 11:58:06 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:06 PM train 005 3.125000e-02 7.500852e-01\n",
      "06/08 11:58:07 PM train_loss 0.036667, train_f1 0.790105\n",
      "06/08 11:58:07 PM epoch 246 lr 3.901024e-03\n",
      "06/08 11:58:07 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:07 PM train 005 3.645833e-02 8.042320e-01\n",
      "06/08 11:58:07 PM train_loss 0.040000, train_f1 0.752833\n",
      "06/08 11:58:07 PM epoch 247 lr 3.761932e-03\n",
      "06/08 11:58:07 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:07 PM train 005 3.125000e-02 8.333759e-01\n",
      "06/08 11:58:07 PM train_loss 0.030000, train_f1 0.862066\n",
      "06/08 11:58:07 PM epoch 248 lr 3.625170e-03\n",
      "06/08 11:58:07 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:08 PM train 005 3.645833e-02 7.375129e-01\n",
      "06/08 11:58:08 PM train_loss 0.040000, train_f1 0.763410\n",
      "06/08 11:58:08 PM epoch 249 lr 3.490753e-03\n",
      "06/08 11:58:08 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:58:08 PM train 005 3.645833e-02 8.042320e-01\n",
      "06/08 11:58:08 PM train_loss 0.036667, train_f1 0.807013\n",
      "06/08 11:58:08 PM epoch 250 lr 3.358695e-03\n",
      "06/08 11:58:08 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:08 PM train 005 2.083333e-02 7.834648e-01\n",
      "06/08 11:58:09 PM train_loss 0.023333, train_f1 0.830123\n",
      "06/08 11:58:09 PM epoch 251 lr 3.229011e-03\n",
      "06/08 11:58:09 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:09 PM train 005 2.083333e-02 9.193974e-01\n",
      "06/08 11:58:09 PM train_loss 0.023333, train_f1 0.913074\n",
      "06/08 11:58:09 PM epoch 252 lr 3.101715e-03\n",
      "06/08 11:58:09 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:09 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:58:09 PM train_loss 0.030000, train_f1 0.859379\n",
      "06/08 11:58:09 PM epoch 253 lr 2.976820e-03\n",
      "06/08 11:58:10 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:10 PM train 005 3.125000e-02 8.221690e-01\n",
      "06/08 11:58:10 PM train_loss 0.036667, train_f1 0.797623\n",
      "06/08 11:58:10 PM epoch 254 lr 2.854342e-03\n",
      "06/08 11:58:10 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:10 PM train 005 2.604167e-02 8.902535e-01\n",
      "06/08 11:58:10 PM train_loss 0.030000, train_f1 0.813633\n",
      "06/08 11:58:10 PM epoch 255 lr 2.734292e-03\n",
      "06/08 11:58:10 PM train 000 6.250000e-02 4.838710e-01\n",
      "06/08 11:58:11 PM train 005 3.125000e-02 7.667989e-01\n",
      "06/08 11:58:11 PM train_loss 0.030000, train_f1 0.819457\n",
      "06/08 11:58:11 PM epoch 256 lr 2.616685e-03\n",
      "06/08 11:58:11 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:11 PM train 005 2.083333e-02 9.083326e-01\n",
      "06/08 11:58:11 PM train_loss 0.023333, train_f1 0.879383\n",
      "06/08 11:58:11 PM epoch 257 lr 2.501532e-03\n",
      "06/08 11:58:11 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:11 PM train 005 2.604167e-02 7.681643e-01\n",
      "06/08 11:58:12 PM train_loss 0.023333, train_f1 0.833159\n",
      "06/08 11:58:12 PM epoch 258 lr 2.388848e-03\n",
      "06/08 11:58:12 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:12 PM train 005 2.604167e-02 8.902535e-01\n",
      "06/08 11:58:12 PM train_loss 0.023333, train_f1 0.904215\n",
      "06/08 11:58:12 PM epoch 259 lr 2.278643e-03\n",
      "06/08 11:58:12 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:12 PM train 005 3.125000e-02 7.500852e-01\n",
      "06/08 11:58:12 PM train_loss 0.026667, train_f1 0.827413\n",
      "06/08 11:58:12 PM epoch 260 lr 2.170931e-03\n",
      "06/08 11:58:12 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:13 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:58:13 PM train_loss 0.030000, train_f1 0.859379\n",
      "06/08 11:58:13 PM epoch 261 lr 2.065723e-03\n",
      "06/08 11:58:13 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:13 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:58:13 PM train_loss 0.026667, train_f1 0.858894\n",
      "06/08 11:58:13 PM epoch 262 lr 1.963031e-03\n",
      "06/08 11:58:13 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:13 PM train 005 2.604167e-02 8.902535e-01\n",
      "06/08 11:58:14 PM train_loss 0.030000, train_f1 0.862066\n",
      "06/08 11:58:14 PM epoch 263 lr 1.862865e-03\n",
      "06/08 11:58:14 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:14 PM train 005 2.604167e-02 7.681643e-01\n",
      "06/08 11:58:14 PM train_loss 0.030000, train_f1 0.783929\n",
      "06/08 11:58:14 PM epoch 264 lr 1.765237e-03\n",
      "06/08 11:58:14 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:14 PM train 005 3.645833e-02 7.209413e-01\n",
      "06/08 11:58:14 PM train_loss 0.033333, train_f1 0.786062\n",
      "06/08 11:58:14 PM epoch 265 lr 1.670157e-03\n",
      "06/08 11:58:15 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:15 PM train 005 1.562500e-02 9.402048e-01\n",
      "06/08 11:58:15 PM train_loss 0.016667, train_f1 0.936183\n",
      "06/08 11:58:15 PM epoch 266 lr 1.577637e-03\n",
      "06/08 11:58:15 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:15 PM train 005 1.562500e-02 9.374765e-01\n",
      "06/08 11:58:15 PM train_loss 0.020000, train_f1 0.928691\n",
      "06/08 11:58:15 PM epoch 267 lr 1.487685e-03\n",
      "06/08 11:58:15 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:16 PM train 005 3.125000e-02 8.500448e-01\n",
      "06/08 11:58:16 PM train_loss 0.026667, train_f1 0.878481\n",
      "06/08 11:58:16 PM epoch 268 lr 1.400313e-03\n",
      "06/08 11:58:16 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:16 PM train 005 2.604167e-02 8.402481e-01\n",
      "06/08 11:58:16 PM train_loss 0.023333, train_f1 0.879293\n",
      "06/08 11:58:16 PM epoch 269 lr 1.315529e-03\n",
      "06/08 11:58:16 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:16 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:58:17 PM train_loss 0.013333, train_f1 0.908702\n",
      "06/08 11:58:17 PM epoch 270 lr 1.233342e-03\n",
      "06/08 11:58:17 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:17 PM train 005 1.562500e-02 9.236331e-01\n",
      "06/08 11:58:17 PM train_loss 0.023333, train_f1 0.888273\n",
      "06/08 11:58:17 PM epoch 271 lr 1.153763e-03\n",
      "06/08 11:58:17 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:17 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:17 PM train_loss 0.016667, train_f1 0.938483\n",
      "06/08 11:58:17 PM epoch 272 lr 1.076799e-03\n",
      "06/08 11:58:17 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:18 PM train 005 1.562500e-02 9.346979e-01\n",
      "06/08 11:58:18 PM train_loss 0.013333, train_f1 0.951311\n",
      "06/08 11:58:18 PM epoch 273 lr 1.002459e-03\n",
      "06/08 11:58:18 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:18 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:58:18 PM train_loss 0.013333, train_f1 0.908702\n",
      "06/08 11:58:18 PM epoch 274 lr 9.307516e-04\n",
      "06/08 11:58:18 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:18 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:19 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:19 PM epoch 275 lr 8.616840e-04\n",
      "06/08 11:58:19 PM train 000 0.000000e+00 1.000000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:58:19 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:19 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:19 PM epoch 276 lr 7.952639e-04\n",
      "06/08 11:58:19 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:19 PM train 005 2.604167e-02 8.347413e-01\n",
      "06/08 11:58:19 PM train_loss 0.026667, train_f1 0.867722\n",
      "06/08 11:58:19 PM epoch 277 lr 7.314987e-04\n",
      "06/08 11:58:19 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:20 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:58:20 PM train_loss 0.013333, train_f1 0.908702\n",
      "06/08 11:58:20 PM epoch 278 lr 6.703953e-04\n",
      "06/08 11:58:20 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:20 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:20 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:20 PM epoch 279 lr 6.119605e-04\n",
      "06/08 11:58:20 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:20 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:21 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:21 PM epoch 280 lr 5.562007e-04\n",
      "06/08 11:58:21 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:21 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:21 PM train_loss 0.013333, train_f1 0.908702\n",
      "06/08 11:58:21 PM epoch 281 lr 5.031220e-04\n",
      "06/08 11:58:21 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:21 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:21 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:21 PM epoch 282 lr 4.527302e-04\n",
      "06/08 11:58:21 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:22 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:22 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:22 PM epoch 283 lr 4.050308e-04\n",
      "06/08 11:58:22 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:22 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:22 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:22 PM epoch 284 lr 3.600291e-04\n",
      "06/08 11:58:22 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:23 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:23 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:23 PM epoch 285 lr 3.177299e-04\n",
      "06/08 11:58:23 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:23 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:23 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:23 PM epoch 286 lr 2.781380e-04\n",
      "06/08 11:58:23 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:23 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:24 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:24 PM epoch 287 lr 2.412577e-04\n",
      "06/08 11:58:24 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:24 PM train 005 1.562500e-02 9.346979e-01\n",
      "06/08 11:58:24 PM train_loss 0.013333, train_f1 0.951311\n",
      "06/08 11:58:24 PM epoch 288 lr 2.070930e-04\n",
      "06/08 11:58:24 PM train 000 3.125000e-02 4.920635e-01\n",
      "06/08 11:58:24 PM train 005 1.562500e-02 8.681209e-01\n",
      "06/08 11:58:24 PM train_loss 0.013333, train_f1 0.908702\n",
      "06/08 11:58:24 PM epoch 289 lr 1.756477e-04\n",
      "06/08 11:58:24 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:25 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:25 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:25 PM epoch 290 lr 1.469252e-04\n",
      "06/08 11:58:25 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:25 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:25 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:25 PM epoch 291 lr 1.209287e-04\n",
      "06/08 11:58:25 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:25 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:26 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:26 PM epoch 292 lr 9.766098e-05\n",
      "06/08 11:58:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:26 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:26 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:26 PM epoch 293 lr 7.712464e-05\n",
      "06/08 11:58:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:26 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:26 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:26 PM epoch 294 lr 5.932192e-05\n",
      "06/08 11:58:26 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:27 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:27 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:27 PM epoch 295 lr 4.425478e-05\n",
      "06/08 11:58:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:27 PM train 005 1.562500e-02 9.346979e-01\n",
      "06/08 11:58:27 PM train_loss 0.013333, train_f1 0.951311\n",
      "06/08 11:58:27 PM epoch 296 lr 3.192486e-05\n",
      "06/08 11:58:27 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:28 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:28 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:28 PM epoch 297 lr 2.233352e-05\n",
      "06/08 11:58:28 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:28 PM train 005 1.562500e-02 9.402048e-01\n",
      "06/08 11:58:28 PM train_loss 0.013333, train_f1 0.954835\n",
      "06/08 11:58:28 PM epoch 298 lr 1.548182e-05\n",
      "06/08 11:58:28 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:28 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:29 PM train_loss 0.010000, train_f1 0.962882\n",
      "06/08 11:58:29 PM epoch 299 lr 1.137049e-05\n",
      "06/08 11:58:29 PM train 000 0.000000e+00 1.000000e+00\n",
      "06/08 11:58:29 PM train 005 1.041667e-02 9.527770e-01\n",
      "06/08 11:58:29 PM train_loss 0.010000, train_f1 0.962882\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    lr = scheduler.get_last_lr()[0]\n",
    "    logging.info('epoch %d lr %e', epoch, lr)\n",
    "    # training\n",
    "    train_loss, train_macro_f1= train(train_queue, model, optimizer)\n",
    "    scheduler.step()\n",
    "    logging.info('train_loss %f, train_f1 %f',train_loss, train_macro_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finished-french",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7582764056752496\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    test_loss, test_macro_f1, array = infer(test_queue, model)\n",
    "print(test_macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee22cd8",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "845e88ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/08 11:58:29 PM NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece0050",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ec481db",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT = np.load(data + 'neuron2_y_test.npy')\n",
    "GT = GT[:, np.newaxis]\n",
    "Ours = np.array(array)\n",
    "Ours = Ours[:, np.newaxis]\n",
    "spike_train = np.concatenate((GT, Ours),axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79809e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x72 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,1))\n",
    "ax = sns.heatmap(spike_train,\n",
    "                 cmap = sns.cubehelix_palette(dark=0.5, light=0.95, as_cmap=True, start= 3.5, rot=-.8),\n",
    "                 yticklabels = ['GT', 'Ours'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6159e",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b856f0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Ours_confusion = confusion_matrix(spike_train[0, :], spike_train[1, :])\n",
    "ax = sns.heatmap(Ours_confusion, annot=True,\n",
    "                 cmap = sns.cubehelix_palette(dark=0.5, light=0.95, as_cmap=True, start= 3.5, rot=-.8),\n",
    "                 vmin=0, vmax=90,\n",
    "                 xticklabels = ['S', 'F'], yticklabels = ['S', 'F'])\n",
    "                 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d7a778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
